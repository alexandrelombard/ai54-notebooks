{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNT/faeT6uGiZ8WFOBU6mLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrelombard/ai54-notebooks/blob/master/Tutorial_3_Word2Vec_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic neural networks"
      ],
      "metadata": {
        "id": "4U0G0kMP1v2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yxG8n3dr1lC2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('height_weight_sex_training_set.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vlJfR43r17K9",
        "outputId": "e3a2a840-4468-406c-c4d7-160e54416fb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Height  Weight     Sex\n",
              "0  165.65   35.41  Female\n",
              "1  148.53   74.45  Female\n",
              "2  167.04   81.22    Male\n",
              "3  161.54   71.47    Male\n",
              "4  174.31   78.18    Male"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff0b2c54-3cd9-4aed-ab13-0920317ca40b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>165.65</td>\n",
              "      <td>35.41</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>148.53</td>\n",
              "      <td>74.45</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>167.04</td>\n",
              "      <td>81.22</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>161.54</td>\n",
              "      <td>71.47</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>174.31</td>\n",
              "      <td>78.18</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff0b2c54-3cd9-4aed-ab13-0920317ca40b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff0b2c54-3cd9-4aed-ab13-0920317ca40b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff0b2c54-3cd9-4aed-ab13-0920317ca40b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7c312bfa-fdf6-454f-9e55-ab8e29f616dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c312bfa-fdf6-454f-9e55-ab8e29f616dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7c312bfa-fdf6-454f-9e55-ab8e29f616dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3000,\n  \"fields\": [\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.91271856414176,\n        \"min\": 25.68,\n        \"max\": 3050.0,\n        \"num_unique_values\": 2330,\n        \"samples\": [\n          146.89,\n          140.33,\n          125.66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.041527655001246,\n        \"min\": 8.53,\n        \"max\": 485.0,\n        \"num_unique_values\": 2310,\n        \"samples\": [\n          93.04,\n          49.18,\n          89.55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the inputs (we want a list of vectors of size 2)\n",
        "heights = torch.tensor(df['Height'], dtype=torch.float32).unsqueeze(1)\n",
        "weights = torch.tensor(df['Weight'], dtype=torch.float32).unsqueeze(1)\n",
        "inputs = torch.cat((heights, weights), dim=1)"
      ],
      "metadata": {
        "id": "hFH6IakM18sZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the outputs (we want 1-hot encoded values for the two possible classes)\n",
        "outputs = F.one_hot(torch.tensor(df['Sex'].replace('Female', 0).replace('Male', 1))).float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Sfzl4q1-AE",
        "outputId": "4735cda6-eab1-45aa-bdbc-52ea1c7d0d0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-261906230.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  outputs = F.one_hot(torch.tensor(df['Sex'].replace('Female', 0).replace('Male', 1))).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model (i.e. the neural network)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 16),\n",
        "    nn.Linear(16, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "aa8NvX801_Rw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 2000\n",
        "for epoch in range(1, epochs + 1):\n",
        "  logits = model(inputs)                # The model is applied on all the inputs\n",
        "  loss = criterion(logits, outputs)     # The error is computed for all the predictions (logits) according to expected outputs\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Every 10 step we print the epoch and the loss so we can see the training\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa_oOIAq2AeQ",
        "outputId": "70fb10cf-0b20-46ca-f8d3-c963dfe14afd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Loss: 4.1945085525512695\n",
            "Epoch: 20, Loss: 0.7244903445243835\n",
            "Epoch: 30, Loss: 0.8891106843948364\n",
            "Epoch: 40, Loss: 0.811125636100769\n",
            "Epoch: 50, Loss: 0.7132465243339539\n",
            "Epoch: 60, Loss: 0.7075966000556946\n",
            "Epoch: 70, Loss: 0.6959497928619385\n",
            "Epoch: 80, Loss: 0.6986805200576782\n",
            "Epoch: 90, Loss: 0.6912343502044678\n",
            "Epoch: 100, Loss: 0.6892827749252319\n",
            "Epoch: 110, Loss: 0.686430037021637\n",
            "Epoch: 120, Loss: 0.6835747957229614\n",
            "Epoch: 130, Loss: 0.6807171106338501\n",
            "Epoch: 140, Loss: 0.677827000617981\n",
            "Epoch: 150, Loss: 0.6748422384262085\n",
            "Epoch: 160, Loss: 0.671753466129303\n",
            "Epoch: 170, Loss: 0.6685654520988464\n",
            "Epoch: 180, Loss: 0.6652814745903015\n",
            "Epoch: 190, Loss: 0.6619141697883606\n",
            "Epoch: 200, Loss: 0.6584964990615845\n",
            "Epoch: 210, Loss: 0.6550701260566711\n",
            "Epoch: 220, Loss: 0.6516693830490112\n",
            "Epoch: 230, Loss: 0.6483179330825806\n",
            "Epoch: 240, Loss: 0.6450304985046387\n",
            "Epoch: 250, Loss: 0.6418138742446899\n",
            "Epoch: 260, Loss: 0.6386656761169434\n",
            "Epoch: 270, Loss: 0.6355686783790588\n",
            "Epoch: 280, Loss: 0.6324822902679443\n",
            "Epoch: 290, Loss: 0.6293565034866333\n",
            "Epoch: 300, Loss: 0.6261597275733948\n",
            "Epoch: 310, Loss: 0.6228769421577454\n",
            "Epoch: 320, Loss: 0.6195021271705627\n",
            "Epoch: 330, Loss: 0.6243228316307068\n",
            "Epoch: 340, Loss: 0.7225813269615173\n",
            "Epoch: 350, Loss: 0.6344181895256042\n",
            "Epoch: 360, Loss: 0.6082770824432373\n",
            "Epoch: 370, Loss: 0.6109215617179871\n",
            "Epoch: 380, Loss: 0.621340811252594\n",
            "Epoch: 390, Loss: 0.6078251600265503\n",
            "Epoch: 400, Loss: 0.669545590877533\n",
            "Epoch: 410, Loss: 0.5934301018714905\n",
            "Epoch: 420, Loss: 0.621767520904541\n",
            "Epoch: 430, Loss: 0.6062621474266052\n",
            "Epoch: 440, Loss: 0.610886812210083\n",
            "Epoch: 450, Loss: 0.5856320261955261\n",
            "Epoch: 460, Loss: 0.5887618064880371\n",
            "Epoch: 470, Loss: 0.644739031791687\n",
            "Epoch: 480, Loss: 0.6847471594810486\n",
            "Epoch: 490, Loss: 0.6640377044677734\n",
            "Epoch: 500, Loss: 0.6372488737106323\n",
            "Epoch: 510, Loss: 0.5865042209625244\n",
            "Epoch: 520, Loss: 0.5681967735290527\n",
            "Epoch: 530, Loss: 0.562734842300415\n",
            "Epoch: 540, Loss: 0.5634254813194275\n",
            "Epoch: 550, Loss: 0.5799885392189026\n",
            "Epoch: 560, Loss: 0.6146623492240906\n",
            "Epoch: 570, Loss: 0.6508554816246033\n",
            "Epoch: 580, Loss: 0.6381710767745972\n",
            "Epoch: 590, Loss: 0.6220847964286804\n",
            "Epoch: 600, Loss: 0.583346962928772\n",
            "Epoch: 610, Loss: 0.5612854957580566\n",
            "Epoch: 620, Loss: 0.5463089346885681\n",
            "Epoch: 630, Loss: 0.5397632718086243\n",
            "Epoch: 640, Loss: 0.5443940162658691\n",
            "Epoch: 650, Loss: 0.5624701976776123\n",
            "Epoch: 660, Loss: 0.5948475003242493\n",
            "Epoch: 670, Loss: 0.6124399304389954\n",
            "Epoch: 680, Loss: 0.6294112205505371\n",
            "Epoch: 690, Loss: 0.5980637073516846\n",
            "Epoch: 700, Loss: 0.5742383599281311\n",
            "Epoch: 710, Loss: 0.5459510684013367\n",
            "Epoch: 720, Loss: 0.5289255380630493\n",
            "Epoch: 730, Loss: 0.5205410718917847\n",
            "Epoch: 740, Loss: 0.5234727263450623\n",
            "Epoch: 750, Loss: 0.5405594110488892\n",
            "Epoch: 760, Loss: 0.5661763548851013\n",
            "Epoch: 770, Loss: 0.6018534898757935\n",
            "Epoch: 780, Loss: 0.5969942212104797\n",
            "Epoch: 790, Loss: 0.5890757441520691\n",
            "Epoch: 800, Loss: 0.5532362461090088\n",
            "Epoch: 810, Loss: 0.5297022461891174\n",
            "Epoch: 820, Loss: 0.5116228461265564\n",
            "Epoch: 830, Loss: 0.5046720504760742\n",
            "Epoch: 840, Loss: 0.5108882784843445\n",
            "Epoch: 850, Loss: 0.5315771698951721\n",
            "Epoch: 860, Loss: 0.5672885179519653\n",
            "Epoch: 870, Loss: 0.5795094966888428\n",
            "Epoch: 880, Loss: 0.5852973461151123\n",
            "Epoch: 890, Loss: 0.5505200028419495\n",
            "Epoch: 900, Loss: 0.5256916284561157\n",
            "Epoch: 910, Loss: 0.503511369228363\n",
            "Epoch: 920, Loss: 0.49349942803382874\n",
            "Epoch: 930, Loss: 0.49757251143455505\n",
            "Epoch: 940, Loss: 0.5180104374885559\n",
            "Epoch: 950, Loss: 0.5542150735855103\n",
            "Epoch: 960, Loss: 0.5660337805747986\n",
            "Epoch: 970, Loss: 0.5684077739715576\n",
            "Epoch: 980, Loss: 0.5337300300598145\n",
            "Epoch: 990, Loss: 0.5091585516929626\n",
            "Epoch: 1000, Loss: 0.4899590313434601\n",
            "Epoch: 1010, Loss: 0.4845809042453766\n",
            "Epoch: 1020, Loss: 0.49681901931762695\n",
            "Epoch: 1030, Loss: 0.5239725708961487\n",
            "Epoch: 1040, Loss: 0.5596277713775635\n",
            "Epoch: 1050, Loss: 0.5530698895454407\n",
            "Epoch: 1060, Loss: 0.538220226764679\n",
            "Epoch: 1070, Loss: 0.5057109594345093\n",
            "Epoch: 1080, Loss: 0.48581022024154663\n",
            "Epoch: 1090, Loss: 0.47788724303245544\n",
            "Epoch: 1100, Loss: 0.4879685938358307\n",
            "Epoch: 1110, Loss: 0.5182449817657471\n",
            "Epoch: 1120, Loss: 0.5413044691085815\n",
            "Epoch: 1130, Loss: 0.55277019739151\n",
            "Epoch: 1140, Loss: 0.5218902230262756\n",
            "Epoch: 1150, Loss: 0.4975895285606384\n",
            "Epoch: 1160, Loss: 0.477905809879303\n",
            "Epoch: 1170, Loss: 0.4735513925552368\n",
            "Epoch: 1180, Loss: 0.48967668414115906\n",
            "Epoch: 1190, Loss: 0.5188410878181458\n",
            "Epoch: 1200, Loss: 0.5478147268295288\n",
            "Epoch: 1210, Loss: 0.5297960042953491\n",
            "Epoch: 1220, Loss: 0.5079733729362488\n",
            "Epoch: 1230, Loss: 0.4818902313709259\n",
            "Epoch: 1240, Loss: 0.4696377217769623\n",
            "Epoch: 1250, Loss: 0.4755358099937439\n",
            "Epoch: 1260, Loss: 0.5009930729866028\n",
            "Epoch: 1270, Loss: 0.5356176495552063\n",
            "Epoch: 1280, Loss: 0.530005693435669\n",
            "Epoch: 1290, Loss: 0.5131347179412842\n",
            "Epoch: 1300, Loss: 0.4844467043876648\n",
            "Epoch: 1310, Loss: 0.46850645542144775\n",
            "Epoch: 1320, Loss: 0.4689667522907257\n",
            "Epoch: 1330, Loss: 0.4905862510204315\n",
            "Epoch: 1340, Loss: 0.5257837176322937\n",
            "Epoch: 1350, Loss: 0.5268785953521729\n",
            "Epoch: 1360, Loss: 0.5132502317428589\n",
            "Epoch: 1370, Loss: 0.48418283462524414\n",
            "Epoch: 1380, Loss: 0.46710994839668274\n",
            "Epoch: 1390, Loss: 0.46577873826026917\n",
            "Epoch: 1400, Loss: 0.4860658347606659\n",
            "Epoch: 1410, Loss: 0.5208017230033875\n",
            "Epoch: 1420, Loss: 0.5229582190513611\n",
            "Epoch: 1430, Loss: 0.5095175504684448\n",
            "Epoch: 1440, Loss: 0.4811945855617523\n",
            "Epoch: 1450, Loss: 0.464739054441452\n",
            "Epoch: 1460, Loss: 0.46450313925743103\n",
            "Epoch: 1470, Loss: 0.4859790802001953\n",
            "Epoch: 1480, Loss: 0.5198861360549927\n",
            "Epoch: 1490, Loss: 0.5184651613235474\n",
            "Epoch: 1500, Loss: 0.5025500655174255\n",
            "Epoch: 1510, Loss: 0.4758814573287964\n",
            "Epoch: 1520, Loss: 0.4617188274860382\n",
            "Epoch: 1530, Loss: 0.46551138162612915\n",
            "Epoch: 1540, Loss: 0.4902208745479584\n",
            "Epoch: 1550, Loss: 0.5215609669685364\n",
            "Epoch: 1560, Loss: 0.5124305486679077\n",
            "Epoch: 1570, Loss: 0.4929298162460327\n",
            "Epoch: 1580, Loss: 0.4691789448261261\n",
            "Epoch: 1590, Loss: 0.4593602418899536\n",
            "Epoch: 1600, Loss: 0.4704051911830902\n",
            "Epoch: 1610, Loss: 0.4985244870185852\n",
            "Epoch: 1620, Loss: 0.5220301747322083\n",
            "Epoch: 1630, Loss: 0.5019605159759521\n",
            "Epoch: 1640, Loss: 0.47957178950309753\n",
            "Epoch: 1650, Loss: 0.46145862340927124\n",
            "Epoch: 1660, Loss: 0.46084997057914734\n",
            "Epoch: 1670, Loss: 0.4848201870918274\n",
            "Epoch: 1680, Loss: 0.5098708271980286\n",
            "Epoch: 1690, Loss: 0.5125812292098999\n",
            "Epoch: 1700, Loss: 0.48329997062683105\n",
            "Epoch: 1710, Loss: 0.46335142850875854\n",
            "Epoch: 1720, Loss: 0.4586288332939148\n",
            "Epoch: 1730, Loss: 0.4800576865673065\n",
            "Epoch: 1740, Loss: 0.5126094222068787\n",
            "Epoch: 1750, Loss: 0.4904409945011139\n",
            "Epoch: 1760, Loss: 0.46069100499153137\n",
            "Epoch: 1770, Loss: 0.4711777865886688\n",
            "Epoch: 1780, Loss: 0.4574423134326935\n",
            "Epoch: 1790, Loss: 0.46036991477012634\n",
            "Epoch: 1800, Loss: 0.5468245148658752\n",
            "Epoch: 1810, Loss: 0.45689982175827026\n",
            "Epoch: 1820, Loss: 0.4755537211894989\n",
            "Epoch: 1830, Loss: 0.4694522023200989\n",
            "Epoch: 1840, Loss: 0.4814787209033966\n",
            "Epoch: 1850, Loss: 0.4867441952228546\n",
            "Epoch: 1860, Loss: 0.4573160707950592\n",
            "Epoch: 1870, Loss: 0.48165544867515564\n",
            "Epoch: 1880, Loss: 0.47654324769973755\n",
            "Epoch: 1890, Loss: 0.4711565673351288\n",
            "Epoch: 1900, Loss: 0.5041162967681885\n",
            "Epoch: 1910, Loss: 0.45758160948753357\n",
            "Epoch: 1920, Loss: 0.48463550209999084\n",
            "Epoch: 1930, Loss: 0.4727504551410675\n",
            "Epoch: 1940, Loss: 0.4715021848678589\n",
            "Epoch: 1950, Loss: 0.5001218318939209\n",
            "Epoch: 1960, Loss: 0.4582451581954956\n",
            "Epoch: 1970, Loss: 0.49923115968704224\n",
            "Epoch: 1980, Loss: 0.4657759666442871\n",
            "Epoch: 1990, Loss: 0.4744420647621155\n",
            "Epoch: 2000, Loss: 0.4882585406303406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on a single example, just to check\n",
        "if model(torch.tensor([150.0, 60.0])).argmax().item() == 0:\n",
        "  print('Female')\n",
        "else:\n",
        "  print('Male')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_jn8MmA2Cun",
        "outputId": "4f1edfac-a024-41bc-845e-d389da3d3a70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec CBOW"
      ],
      "metadata": {
        "id": "rRYSFyw32VIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV4aXGNx2b5S",
        "outputId": "9e6f54d0-479a-4f8a-de84-82d37d5b19cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the right device can speed up the computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use the GPU if available\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrrKVik22foi",
        "outputId": "3fadfd84-de51-44be-fee3-47cd4192e7e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('romeo_and_juliet.txt') as file:\n",
        "  content = file.read()"
      ],
      "metadata": {
        "id": "tXIo6JHM2hUj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing of the text\n",
        "content = content.lower()\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "# Remove punctuation\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "print(tokens[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlPfemkX2nMg",
        "outputId": "57208f97-6642-4929-bdb2-f07dccf6d71d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'tragedy', 'of', 'romeo', 'and', 'juliet', 'by', 'william', 'shakespeare', 'dramatis', 'personae', 'chorus', 'escalus', 'prince', 'of', 'verona', 'paris', 'a', 'young', 'count', 'kinsman', 'to', 'the', 'prince', 'montague', 'heads', 'of', 'two', 'houses', 'at', 'variance', 'with', 'each', 'other', 'capulet', 'heads', 'of', 'two', 'houses', 'at', 'variance', 'with', 'each', 'other', 'an', 'old', 'man', 'of', 'the', 'capulet', 'family', 'romeo', 'son', 'to', 'montague', 'tybalt', 'nephew', 'to', 'lady', 'capulet', 'mercutio', 'kinsman', 'to', 'the', 'prince', 'and', 'friend', 'to', 'romeo', 'benvolio', 'nephew', 'to', 'montague', 'and', 'friend', 'to', 'romeo', 'tybalt', 'nephew', 'to', 'lady', 'capulet', 'friar', 'laurence', 'franciscan', 'friar', 'john', 'franciscan', 'balthasar', 'servant', 'to', 'romeo', 'abram', 'servant', 'to', 'montague', 'sampson', 'servant', 'to', 'capulet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a vocabulary and a dictionary so we have indices for each word\n",
        "vocabulary = list(set(tokens))\n",
        "\n",
        "word2idx = {}\n",
        "for i in range(len(vocabulary)):\n",
        "  word2idx[vocabulary[i]] = i\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq5gUl_93I8a",
        "outputId": "2b72bc40-6ca5-401e-ae29-177d32dc617c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the dataset\n",
        "target_word_ids = []\n",
        "context_words_ids = []\n",
        "\n",
        "for position in range(2, len(tokens) - 2):\n",
        "  target_word_ids.append(word2idx[tokens[position]])\n",
        "  context_words_ids.append([\n",
        "      word2idx[tokens[position-2]],\n",
        "      word2idx[tokens[position-1]],\n",
        "      word2idx[tokens[position+1]],\n",
        "      word2idx[tokens[position+2]]\n",
        "    ])"
      ],
      "metadata": {
        "id": "wV76ZNIW3svq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Word2Vec CBOW module\n",
        "class Word2VecCBOW(nn.Module):\n",
        "  def __init__(self, vocabulary_size, embedding_dim):\n",
        "    super(Word2VecCBOW, self).__init__()\n",
        "    # An embedding layer, to reduce the size of vectors from vocabulary size to the embedding dimension\n",
        "    self.embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "    # An output layer, to have the probabilities for each target words from the embedding\n",
        "    self.linear = nn.Linear(embedding_dim, vocabulary_size, bias=False)\n",
        "\n",
        "  def forward(self, context):\n",
        "    # Computing the embedding for the context words\n",
        "    embed = self.embeddings(context)\n",
        "    # Make an aggregation\n",
        "    sum_embed = torch.sum(embed, dim=1)\n",
        "    # Compute the output\n",
        "    out = self.linear(sum_embed)\n",
        "\n",
        "    return out\n",
        "\n",
        "word2vec_cbow = Word2VecCBOW(len(vocabulary), 128).to(device)"
      ],
      "metadata": {
        "id": "6Rqwddya3TR6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(word2vec_cbow.parameters())"
      ],
      "metadata": {
        "id": "AFhab04f4Tj5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (it can be really slow, there is no optimization here except training with batches)\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "  batch_size = 2000 # We compute the loss on batches of 2000 elements, to speed up the training process\n",
        "  for position in range(0, len(tokens) - batch_size, batch_size):\n",
        "    batch_input = context_words_ids[position:position + batch_size]\n",
        "    batch_output = target_word_ids[position:position + batch_size]\n",
        "\n",
        "    prediction = word2vec_cbow(torch.tensor(batch_input, device=device))\n",
        "    loss = criterion(prediction, torch.tensor(batch_output, device=device))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  print(f'Epoch #{epoch}, avg loss {torch.mean(torch.tensor(losses)).item()}')\n",
        "  losses.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dj3BF9b4dRH",
        "outputId": "092423a1-ab8e-4585-e7ee-586a23ec3b0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0, avg loss 1.3378357887268066\n",
            "Epoch #1, avg loss 1.325903058052063\n",
            "Epoch #2, avg loss 1.314109444618225\n",
            "Epoch #3, avg loss 1.3024535179138184\n",
            "Epoch #4, avg loss 1.2909343242645264\n",
            "Epoch #5, avg loss 1.279550313949585\n",
            "Epoch #6, avg loss 1.2683005332946777\n",
            "Epoch #7, avg loss 1.2571839094161987\n",
            "Epoch #8, avg loss 1.2461987733840942\n",
            "Epoch #9, avg loss 1.2353442907333374\n",
            "Epoch #10, avg loss 1.2246193885803223\n",
            "Epoch #11, avg loss 1.2140225172042847\n",
            "Epoch #12, avg loss 1.2035528421401978\n",
            "Epoch #13, avg loss 1.1932088136672974\n",
            "Epoch #14, avg loss 1.1829897165298462\n",
            "Epoch #15, avg loss 1.1728936433792114\n",
            "Epoch #16, avg loss 1.1629201173782349\n",
            "Epoch #17, avg loss 1.1530675888061523\n",
            "Epoch #18, avg loss 1.143335223197937\n",
            "Epoch #19, avg loss 1.1337225437164307\n",
            "Epoch #20, avg loss 1.1242305040359497\n",
            "Epoch #21, avg loss 1.1148566007614136\n",
            "Epoch #22, avg loss 1.105603814125061\n",
            "Epoch #23, avg loss 1.096430778503418\n",
            "Epoch #24, avg loss 1.0873879194259644\n",
            "Epoch #25, avg loss 1.0784589052200317\n",
            "Epoch #26, avg loss 1.0696403980255127\n",
            "Epoch #27, avg loss 1.0609309673309326\n",
            "Epoch #28, avg loss 1.0523290634155273\n",
            "Epoch #29, avg loss 1.0438337326049805\n",
            "Epoch #30, avg loss 1.0354433059692383\n",
            "Epoch #31, avg loss 1.0271570682525635\n",
            "Epoch #32, avg loss 1.0189732313156128\n",
            "Epoch #33, avg loss 1.0108911991119385\n",
            "Epoch #34, avg loss 1.0029090642929077\n",
            "Epoch #35, avg loss 0.9950262904167175\n",
            "Epoch #36, avg loss 0.9872410297393799\n",
            "Epoch #37, avg loss 0.9795527458190918\n",
            "Epoch #38, avg loss 0.9719597697257996\n",
            "Epoch #39, avg loss 0.9644608497619629\n",
            "Epoch #40, avg loss 0.9570549130439758\n",
            "Epoch #41, avg loss 0.9497408866882324\n",
            "Epoch #42, avg loss 0.9425173401832581\n",
            "Epoch #43, avg loss 0.935383141040802\n",
            "Epoch #44, avg loss 0.9283372759819031\n",
            "Epoch #45, avg loss 0.9213784337043762\n",
            "Epoch #46, avg loss 0.914505660533905\n",
            "Epoch #47, avg loss 0.9077175259590149\n",
            "Epoch #48, avg loss 0.9010129570960999\n",
            "Epoch #49, avg loss 0.894390881061554\n",
            "Epoch #50, avg loss 0.8878503441810608\n",
            "Epoch #51, avg loss 0.8813900351524353\n",
            "Epoch #52, avg loss 0.8750086426734924\n",
            "Epoch #53, avg loss 0.868705689907074\n",
            "Epoch #54, avg loss 0.8624793887138367\n",
            "Epoch #55, avg loss 0.8563290238380432\n",
            "Epoch #56, avg loss 0.8502534031867981\n",
            "Epoch #57, avg loss 0.8442516326904297\n",
            "Epoch #58, avg loss 0.8383224010467529\n",
            "Epoch #59, avg loss 0.8324651122093201\n",
            "Epoch #60, avg loss 0.8266782164573669\n",
            "Epoch #61, avg loss 0.820961058139801\n",
            "Epoch #62, avg loss 0.815312385559082\n",
            "Epoch #63, avg loss 0.809731662273407\n",
            "Epoch #64, avg loss 0.8042173385620117\n",
            "Epoch #65, avg loss 0.798768937587738\n",
            "Epoch #66, avg loss 0.7933850884437561\n",
            "Epoch #67, avg loss 0.7880653738975525\n",
            "Epoch #68, avg loss 0.7828083038330078\n",
            "Epoch #69, avg loss 0.7776133418083191\n",
            "Epoch #70, avg loss 0.7724793553352356\n",
            "Epoch #71, avg loss 0.7674058079719543\n",
            "Epoch #72, avg loss 0.7623915076255798\n",
            "Epoch #73, avg loss 0.7574356198310852\n",
            "Epoch #74, avg loss 0.7525375485420227\n",
            "Epoch #75, avg loss 0.7476963400840759\n",
            "Epoch #76, avg loss 0.7429108619689941\n",
            "Epoch #77, avg loss 0.7381808161735535\n",
            "Epoch #78, avg loss 0.7335049510002136\n",
            "Epoch #79, avg loss 0.7288828492164612\n",
            "Epoch #80, avg loss 0.724313497543335\n",
            "Epoch #81, avg loss 0.7197964191436768\n",
            "Epoch #82, avg loss 0.7153304219245911\n",
            "Epoch #83, avg loss 0.7109155654907227\n",
            "Epoch #84, avg loss 0.7065510749816895\n",
            "Epoch #85, avg loss 0.702237069606781\n",
            "Epoch #86, avg loss 0.6979718804359436\n",
            "Epoch #87, avg loss 0.6937534809112549\n",
            "Epoch #88, avg loss 0.6895774006843567\n",
            "Epoch #89, avg loss 0.685448169708252\n",
            "Epoch #90, avg loss 0.6813650727272034\n",
            "Epoch #91, avg loss 0.6773285865783691\n",
            "Epoch #92, avg loss 0.6733370423316956\n",
            "Epoch #93, avg loss 0.6693909764289856\n",
            "Epoch #94, avg loss 0.6654906272888184\n",
            "Epoch #95, avg loss 0.6616392135620117\n",
            "Epoch #96, avg loss 0.6578202247619629\n",
            "Epoch #97, avg loss 0.6540428996086121\n",
            "Epoch #98, avg loss 0.6502940654754639\n",
            "Epoch #99, avg loss 0.6465962529182434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # Disable the computation of gradients (useful for evaluation)\n",
        "  # The weights of the embedding matrix are the embeddings of all words (the line #0 is embedding of word #0, etc.)\n",
        "  embeddings = word2vec_cbow.embeddings.weight.detach()\n",
        "  # Here we normalize the embeddings to be able to compute the cosine similarity by taking the dot product of embeddings\n",
        "  normalized_embeddings = embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
        "\n",
        "  embedding = normalized_embeddings[word2idx['man']]\n",
        "\n",
        "  # Efficiently compute the dot product of all lines of normalized_embeddings with embedding\n",
        "  similarities = torch.mv(normalized_embeddings, embedding)\n",
        "\n",
        "  # Get the 10 top indices (discard the values)\n",
        "  _, top10 = torch.topk(similarities, 10, largest=True, sorted=True)\n",
        "\n",
        "  print([vocabulary[idx.item()] for idx in top10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZjC6swb7HqJ",
        "outputId": "abb6e8b7-7c13-4b36-ed76-7e2ff35bbabb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['man', 'streaks', 'collars', 'sycamore', 'wings', 'italy', 'guest', 'our', 'drunk', 'desirest']\n"
          ]
        }
      ]
    }
  ]
}